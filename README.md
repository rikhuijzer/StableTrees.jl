# StableTrees.jl

This package implements the **S**table and **I**nterpretable **RU**le **S**ets (SIRUS) for binary classification.
Regression and multiclass-classification are also technically possible but not yet implemented.
The focus is on binary classification because I need that for a paper.

The SIRUS algorithm was presented by Bénard et al. in 2020 and 2021.
In short, SIRUS combines the predictive accuracy of random forests with the explainability of decision trees while remaining stable.
Decision trees are easily interpretable but are unstable, meaning that small changes in the dataset can change the model drastically.
Random forests have solved this by fitting multiple trees.
However, interpretability of random forests is limited even with tools such as Shapley values.
For example, it is not possible to reconstruct the model given only the Shapley values.
This package solves these problems by finding a number of decision rules; typically 10.

Note that, compared to random forests, the accuracy of these rules are lower than the accuracy of the pure tree.
However, in practise, it is very likely that the accuracy of the rules is higher because the model can be verified by interpreting the rules.

## Algorithm

For more information about the algorithm, see Bénard et al. in 2020 (classification) and 2021 (regression).

### Rule generation

These rules are stable because of a small modification to the original random forest algorithm.
Specifically, the forest structure is restricted by pre-defining data-based quantiles.
These quantiles remain reasonably stable for data pertubations and, therefore, cause the splitpoints to be stable.
Next, note that each node of each tree defines a hyperrectangle in the input space.
Each hyperrectangle can be converted to a rule based on whether a query point falls into such the hyperrectangle.
Typically, this process generates 10k rules.

### Rule selection

There is usually large overlap in the rules generated by different trees.
The idea is to deselect those rules that appear less frequent in the finite list of all possible paths.

### Rule set post-treatment

In the post-treatment, all rules are removed which are a linear combination of rules associated with higher frequency paths.
This results in a small set of regression rules.

### Rule aggregation

Next, if a point falls into the corresponding hyperrectangle, then return the average of the outcomes for the training points.
If a point falls outside the corresponding hyperrectangle, then return the average outcome for the points not associated with the hyperrectangle.
Finally, each rule obtains a non-negative weight in order to combine them into a single estimate.
